# -*- coding: utf-8 -*-
"""TrustscoreShap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-Qmkz2NRiyCbJ31nTmTjZIabPsq926b
"""

# -*- coding: utf-8 -*-
"""Trustscore v2 with SHAP.ipynb
Automatically generated and SHAP-enhanced for Colab.
"""

!pip install shap

# ========== ðŸ” IMPORTS ========== #
import pandas as pd
import shap
import joblib
import numpy as np

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from google.colab import files

from trustscore_batch_analyzer import TrustScoreAnalyzer, create_sample_data

# ========== ðŸ› ï¸ GENERATE DATA ========== #
df = create_sample_data(5000)
print(f"âœ… Generated dataset with shape: {df.shape}")
df.to_csv("generated_trustscore_data.csv", index=False)
print("ðŸ’¾ Dataset saved to generated_trustscore_data.csv")

# ========== ðŸ” RUN TRUSTSCORE ANALYZER ========== #
analyzer = TrustScoreAnalyzer()
results_df = analyzer.analyze_batch(df)
summary = analyzer.generate_summary_report(results_df)

print("\n=== ðŸ” ANALYSIS SUMMARY ===")
print(f"ðŸ“Š Total Records Processed: {summary['total_records']}")
print(f"âœ… Overall Approval Rate: {summary['approval_rate']:.1f}%")
print(f"âš ï¸ High Risk Applications: {summary['high_risk_count']}")

print("\n=== ðŸ“‚ DECISION BREAKDOWN ===")
for decision, count in summary['decision_distribution'].items():
    percentage = summary['decision_percentages'][decision]
    print(f"  {decision}: {count} ({percentage}%)")

print("\n=== ðŸ“ˆ SCORE STATISTICS ===")
score_stats = summary['score_statistics']['overall_score']
print(f"  Average Score: {score_stats['mean']:.1f}")
print(f"  Score Range: {score_stats['min']:.0f} - {score_stats['max']:.0f}")

print("\nTop 5 Highest Scores:")
display(results_df.nlargest(5, 'overall_score')[['overall_score', 'decision', 'risk_category', 'income', 'credit_score']])

print("Top 5 Lowest Scores:")
display(results_df.nsmallest(5, 'overall_score')[['overall_score', 'decision', 'risk_category', 'income', 'credit_score']])

print("\nðŸ“Š Generating visualizations...")
analyzer.create_visualizations(results_df)

results_df.to_csv("trustscore_results_5000.csv", index=False)
print("ðŸ’¾ Results saved to trustscore_results_5000.csv")
files.download("trustscore_results_5000.csv")

# ========== ðŸ¤– TRAIN MODEL ========== #
features = [
    'income', 'credit_score', 'loan_amount', 'loan_term', 'age', 'monthly_debt',
    'employment', 'loan_purpose', 'gender', 'residence'
]

X = pd.get_dummies(results_df[features], drop_first=True)
y = results_df['decision']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)

print("âœ… Retrained Classification Report:\n")
print(classification_report(y_test, rf_model.predict(X_test)))

print("\nðŸ“‰ Confusion Matrix:")
print(confusion_matrix(y_test, rf_model.predict(X_test)))

# ========== ðŸ’¾ EXPORT MODEL ========== #
model_path = "trustscore_rf_model_5000.pkl"
joblib.dump(rf_model, model_path)
print(f"ðŸ’¾ Model saved to {model_path}")
files.download(model_path)

# ========== ðŸ”® SAMPLE PREDICTIONS ========== #
df_new = create_sample_data(5)
df_new_encoded = pd.get_dummies(df_new[features], drop_first=True)
df_new_encoded = df_new_encoded.reindex(columns=X.columns, fill_value=0)
new_predictions = rf_model.predict(df_new_encoded)
df_new['predicted_decision'] = new_predictions
print("\nðŸ”® New Predictions:")
print(df_new[['income', 'credit_score', 'loan_amount', 'employment', 'loan_purpose', 'predicted_decision']])

# ========== ðŸ” SHAP EXPLAINABILITY ========== #
print("
ðŸ” SHAP Analysis Starting...")
shap.initjs()

# Sample 5 instances
sample = X_test.iloc[:5]

# SHAP Explainer
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(sample)

target_index = 1 if len(shap_values) > 1 else 0

# Use HTML-based force plot (Colab-friendly)
shap_html = shap.force_plot(
    explainer.expected_value[target_index],
    shap_values[target_index][0, :],
    sample.iloc[0, :],
    feature_names=sample.columns.tolist(),
    show=False
)
from IPython.core.display import display, HTML
display(HTML(shap_html.html()))

# Print human-readable SHAP explanations
print("
ðŸ§  SHAP Explanation in Words:")
for i in range(len(sample)):
    print(f"
ðŸ“Œ Sample #{i+1} prediction: {rf_model.predict([sample.iloc[i]])[0]}")
    shap_values_i = shap_values[target_index][i]
    top_features = np.argsort(-np.abs(shap_values_i))[:3]
    print("Top features driving the prediction:")
    for j in top_features:
        feature_name = sample.columns[j]
        impact = shap_values_i[j]
        direction = "increased" if impact > 0 else "decreased"
        print(f" - {feature_name} {direction} the likelihood of approval by {abs(impact):.4f}")

# SHAP Summary Plot

except Exception as e:
    print("âš ï¸ Failed to render summary plot:", e)
try:
    print("
ðŸ“Š SHAP Summary Plot:")
    shap.summary_plot(shap_values[target_index], sample, feature_names=sample.columns.tolist())
except Exception as e:
    print("âš ï¸ Failed to render summary plot:", e)

# -*- coding: utf-8 -*-
"""Trustscore v2 with SHAP.ipynb
Automatically generated and SHAP-enhanced for Colab.
"""

!pip install shap

# ========== ðŸ” IMPORTS ========== #
import pandas as pd
import shap
import joblib
import numpy as np

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from google.colab import files

from trustscore_batch_analyzer import TrustScoreAnalyzer, create_sample_data

# ========== ðŸ› ï¸ GENERATE DATA ========== #
df = create_sample_data(5000)
print(f"âœ… Generated dataset with shape: {df.shape}")
df.to_csv("generated_trustscore_data.csv", index=False)
print("ðŸ’¾ Dataset saved to generated_trustscore_data.csv")

# ========== ðŸ” RUN TRUSTSCORE ANALYZER ========== #
analyzer = TrustScoreAnalyzer()
results_df = analyzer.analyze_batch(df)
summary = analyzer.generate_summary_report(results_df)

print("\n=== ðŸ” ANALYSIS SUMMARY ===")
print(f"ðŸ“Š Total Records Processed: {summary['total_records']}")
print(f"âœ… Overall Approval Rate: {summary['approval_rate']:.1f}%")
print(f"âš ï¸ High Risk Applications: {summary['high_risk_count']}")

print("\n=== ðŸ“‚ DECISION BREAKDOWN ===")
for decision, count in summary['decision_distribution'].items():
    percentage = summary['decision_percentages'][decision]
    print(f"  {decision}: {count} ({percentage}%)")

print("\n=== ðŸ“ˆ SCORE STATISTICS ===")
score_stats = summary['score_statistics']['overall_score']
print(f"  Average Score: {score_stats['mean']:.1f}")
print(f"  Score Range: {score_stats['min']:.0f} - {score_stats['max']:.0f}")

print("\nTop 5 Highest Scores:")
display(results_df.nlargest(5, 'overall_score')[['overall_score', 'decision', 'risk_category', 'income', 'credit_score']])

print("Top 5 Lowest Scores:")
display(results_df.nsmallest(5, 'overall_score')[['overall_score', 'decision', 'risk_category', 'income', 'credit_score']])

print("\nðŸ“Š Generating visualizations...")
analyzer.create_visualizations(results_df)

results_df.to_csv("trustscore_results_5000.csv", index=False)
print("ðŸ’¾ Results saved to trustscore_results_5000.csv")
files.download("trustscore_results_5000.csv")

# ========== ðŸ¤– TRAIN MODEL ========== #
features = [
    'income', 'credit_score', 'loan_amount', 'loan_term', 'age', 'monthly_debt',
    'employment', 'loan_purpose', 'gender', 'residence'
]

X = pd.get_dummies(results_df[features], drop_first=True)
y = results_df['decision']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)

print("âœ… Retrained Classification Report:\n")
print(classification_report(y_test, rf_model.predict(X_test)))

print("\nðŸ“‰ Confusion Matrix:")
print(confusion_matrix(y_test, rf_model.predict(X_test)))

# ========== ðŸ’¾ EXPORT MODEL ========== #
model_path = "trustscore_rf_model_5000.pkl"
joblib.dump(rf_model, model_path)
print(f"ðŸ’¾ Model saved to {model_path}")
files.download(model_path)

# ========== ðŸ”® SAMPLE PREDICTIONS ========== #
df_new = create_sample_data(5)
df_new_encoded = pd.get_dummies(df_new[features], drop_first=True)
df_new_encoded = df_new_encoded.reindex(columns=X.columns, fill_value=0)
new_predictions = rf_model.predict(df_new_encoded)
df_new['predicted_decision'] = new_predictions
print("\nðŸ”® New Predictions:")
print(df_new[['income', 'credit_score', 'loan_amount', 'employment', 'loan_purpose', 'predicted_decision']])

# ========== ðŸ” SHAP EXPLAINABILITY ========== #
print("\nðŸ” SHAP Analysis Starting...")
shap.initjs()

# Sample 5 instances
sample = X_test.iloc[:5].reindex(columns=X.columns, fill_value=0)

# SHAP Explainer
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(sample)

# Visualize first sample
shap.force_plot(explainer.expected_value[1], shap_values[1][0, :], sample.iloc[0, :], matplotlib=True)

# Print human-readable SHAP explanations
print("\nðŸ§  SHAP Explanation in Words:")
for i in range(len(sample)):
    print(f"\nðŸ“Œ Sample #{i+1} prediction: {rf_model.predict([sample.iloc[i]])[0]}")
    shap_values_i = shap_values[1][i]
    top_features = np.argsort(-np.abs(shap_values_i))[:3]
    print("Top features driving the prediction:")
    for j in top_features:
        feature_name = sample.columns[j]
        impact = shap_values_i[j]
        direction = "increased" if impact > 0 else "decreased"
        print(f" - {feature_name} {direction} the likelihood of approval by {abs(impact):.4f}")

# Optional: SHAP Summary Plot
# print("\nðŸ“Š SHAP Summary Plot:")
# shap.summary_plot(shap_values[1], sample, feature_names=sample.columns)